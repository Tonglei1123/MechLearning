{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "\n",
    "简单的TensorFlow可以简单替代 NumPy，但复杂级别的项目还是要专门的TensorFlow。\n",
    "主要区别：\n",
    "– 随机数策略不同（tf 有全局/局部随机生成器，np 用 RandomState）；\n",
    "– 整数除法默认行为不同（tf 和 Python3 一致，np 默认截断除法）；\n",
    "– 布尔型索引写法不同（tf 用 tf.boolean_mask）；\n",
    "– 可变长度循环需用 tf.while_loop/tf.scan\n",
    "\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "\n",
    "形状、dtype、数值都一样，但：\n",
    "• tf.range 在图里用 Range 算子，不占用内存常量区；\n",
    "• tf.constant(np.arange(10)) 会把 10 个整数拷进图常量，图文件会大 10×4/8 B。\n",
    "tf.range(10)：\n",
    "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>\n",
    "tf.constant(np.arange(10))：\n",
    "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\n",
    "\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "\n",
    "• 函数：\n",
    "– 仅依赖 y_true/y_pred（以及外部闭包里可序列化的超参）；\n",
    "– 无需保存配置，一行写完；\n",
    "– 典型场景：交叉熵＋权重、Focal loss、Dice loss。\n",
    "• 子类：\n",
    "– 需要保存/恢复超参（如 α、γ、margin）；\n",
    "– 想在 .compile() 里只传类名，让 Keras 自动实例化。\n",
    "\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "\n",
    "• 函数：\n",
    "– 每个 batch 返回一个标量即可，不跨批次累加；\n",
    "– 适合“一次性”指标，如当前 batch 的 top-3 accuracy。\n",
    "• 子类：\n",
    "– 必须跨批次累积（accuracy、AUC、F1）；\n",
    "– 需要 .update_state()、.result()、.reset_states()；\n",
    "– 希望 model.fit 自动调用。\n",
    "\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "\n",
    "Layer = “张量 → 张量”的可复用积木，无外部数据依赖，可放入任意模型；\n",
    "Model = 端到端有输入/输出的完整图，可 .fit/.predict/.save。\n",
    "\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "\n",
    "对抗训练（GAN、FGSM、PGD）——需要交替更新两组网络；\n",
    "强化学习——每步环境交互后才产生标签，批次大小不一；\n",
    "知识蒸馏——教师模型输出当软标签，需要中间特征对齐；\n",
    "多任务/多阶段——不同任务用不同优化器、学习率、梯度裁剪；\n",
    "梯度累积、混合精度、梯度检查点等需手动控制 tf.GradientTape()；\n",
    "非标准日志（需要把每步的梯度范数、参数直方图写回 TensorBoard）\n",
    "\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "\n",
    "• 能放，但只在 eager 下永远 OK；\n",
    "• 一旦 @tf.function 装饰（或 model.fit 默认 autograph），必须能被 AutoGraph 识别/转换，否则：\n",
    "– 报错（py 内置 list 插入、非 tf 控制流、外部 I/O）；\n",
    "– 或 silently 回退到 py func，图断裂，性能暴跌。\n",
    "写组件时，把“训练相关计算”全用 tf 原生算子\n",
    "\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "\n",
    "• 在 tf.function 内修改外部 Python list/dict 等非 tf.Variable；\n",
    "• 用 Python 的 for/while 遍历 Tensor 第一维（应改用 tf.range+tf.while_loop）；\n",
    "• 用 if tensor: 判断（应 if tf.reduce_sum(tensor) > 0:）；\n",
    "• 在图里放 print()/time.sleep()/input() 等阻塞 I/O；\n",
    "• 用 numpy() 把张量拖回 CPU（会触发图外拷贝，直接报错）；\n",
    "• 依赖全局随机状态（np.random、random模块），应改用 tf.random.* 并传入 seed。\n",
    "\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n",
    "\n",
    "• 输入序列长度每次都变，且后续层要用到该长度（如 Transformer 的 mask）；\n",
    "• 每次训练样本数量不同，或需要动态 RNN 展开；\n",
    "• 网络拓扑本身随输入而变（动态点云、图神经网络、Neural Architecture Search 一次采一个子图）。\n"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "484d5fe1bd4fc1dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:00:24.066708Z",
     "start_time": "2025-09-13T04:00:24.061073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dense层的简化自定义实现  # 形状 输出的长度，激活函数， W， b\n",
    "import tensorflow as tf\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf,keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": tf.keras.activations.serialize(self.activation)}"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:03:49.269201Z",
     "start_time": "2025-09-13T04:03:49.262923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 执行层归一化的自定义层\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    逐样本、逐特征维度的 Layer Normalization 自定义层。\n",
    "    公式:\n",
    "        y = α ⊙ (x - μ) / (σ + ε) + β\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon: float = 1e-3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        \"\"\"\n",
    "        定义两个可训练权重：\n",
    "           α 初始化为 1\n",
    "           β 初始化为 0\n",
    "           形状均为 input_shape[-1:]\n",
    "        \"\"\"\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=batch_input_shape[-1:],\n",
    "            initializer='ones')\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta',\n",
    "            shape=batch_input_shape[-1:],\n",
    "            initializer='zeros')\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        计算每个实例特征的均值与标准差，并返回归一化结果\n",
    "        \"\"\"\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / tf.sqrt(variance + self.epsilon) + self.beta\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, 'epsilon': self.epsilon}"
   ],
   "id": "dc0504e04997e1aa",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:00:27.393883Z",
     "start_time": "2025-09-13T04:00:27.367247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = tf.random.normal((64,64,64))\n",
    "offical_LayerNormalization = tf.keras.layers.LayerNormalization(epsilon=1e-3)\n",
    "own_LayerNormalization = LayerNormalization(epsilon=1e-3)\n",
    "\n",
    "y1 = offical_LayerNormalization(x, training=False)\n",
    "y2 = own_LayerNormalization(x, training=False)\n",
    "\n",
    "diff = tf.reduce_max(y1 - y2)\n",
    "print(\"max_diff\", tf.abs(diff).numpy())"
   ],
   "id": "d8d5817ebb1bedcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff 7.1525574e-07\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53fa4f011cf83f51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T01:24:34.654855Z",
     "start_time": "2025-09-15T01:24:34.650989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n"
   ],
   "id": "70c1b3ab0713840c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:11.343751Z",
     "start_time": "2025-09-13T04:05:11.172363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
    "x_test  = tf.cast(x_test,  tf.float32) / 255.0\n",
    "x_train = tf.expand_dims(x_train, -1)   # (60000, 28, 28, 1)\n",
    "x_test  = tf.expand_dims(x_test,  -1)"
   ],
   "id": "43e9deb655415132",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:12.407476Z",
     "start_time": "2025-09-13T04:05:12.396456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "          .shuffle(60000).batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "          .batch(128).prefetch(tf.data.AUTOTUNE)"
   ],
   "id": "7b5c00a89478c6ed",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:13.598904Z",
     "start_time": "2025-09-13T04:05:13.566190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28, 1)),\n",
    "    layers.Dense(512, name=\"dense1\"),\n",
    "    LayerNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.Dense(256, name=\"dense2\"),\n",
    "    LayerNormalization(),\n",
    "    layers.ReLU(),\n",
    "    layers.Dense(10, name=\"logits\")\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ],
   "id": "2e3e66f4c6363a94",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:15.129466Z",
     "start_time": "2025-09-13T04:05:15.121286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bottom_vars, top_vars = [], []\n",
    "for layer in model.layers:\n",
    "    if layer.name in {\"dense1\", \"dense2\"} or isinstance(layer, LayerNormalization):\n",
    "        bottom_vars += layer.trainable_variables\n",
    "    elif layer.name == \"logits\":\n",
    "        top_vars += layer.trainable_variables\n",
    "\n",
    "opt_bottom = tf.keras.optimizers.Adam(1e-4)\n",
    "opt_top    = tf.keras.optimizers.SGD(1e-2)"
   ],
   "id": "5b3671f69dd4e6f4",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:16.325609Z",
     "start_time": "2025-09-13T04:05:16.316844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tf.function\n",
    "def train_step(x, y, train_loss, train_acc):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss   = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss, bottom_vars + top_vars)\n",
    "    g_bottom, g_top = grads[:len(bottom_vars)], grads[len(bottom_vars):]\n",
    "    opt_bottom.apply_gradients(zip(g_bottom, bottom_vars))\n",
    "    opt_top.apply_gradients(zip(g_top, top_vars))\n",
    "    train_loss.update_state(loss)\n",
    "    train_acc.update_state(y, logits)\n",
    "\n",
    "@tf.function\n",
    "def val_step(x, y, val_loss, val_acc):\n",
    "    logits = model(x, training=False)\n",
    "    loss   = loss_fn(y, logits)\n",
    "    val_loss.update_state(loss)\n",
    "    val_acc.update_state(y, logits)"
   ],
   "id": "d255c53b003be841",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-13T04:05:59.046478Z",
     "start_time": "2025-09-13T04:05:18.077386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ★★★ 每 epoch 新建一套指标 ★★★\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    train_acc  = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_loss   = tf.keras.metrics.Mean()\n",
    "    val_acc    = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # 训练\n",
    "    for x, y in train_ds:\n",
    "        train_step(x, y, train_loss, train_acc)\n",
    "\n",
    "    # 验证\n",
    "    for x, y in val_ds:\n",
    "        val_step(x, y, val_loss, val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}  \"\n",
    "          f\"train_loss {train_loss.result():.4f}  train_acc {train_acc.result():.4f}  \"\n",
    "          f\"val_loss {val_loss.result():.4f}  val_acc {val_acc.result():.4f}\")\n"
   ],
   "id": "13f4ff97cf705450",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  train_loss 0.4067  train_acc 0.8871  val_loss 0.1806  val_acc 0.9495\n",
      "Epoch 2  train_loss 0.1479  train_acc 0.9589  val_loss 0.1270  val_acc 0.9623\n",
      "Epoch 3  train_loss 0.1016  train_acc 0.9723  val_loss 0.1002  val_acc 0.9705\n",
      "Epoch 4  train_loss 0.0757  train_acc 0.9801  val_loss 0.0881  val_acc 0.9739\n",
      "Epoch 5  train_loss 0.0575  train_acc 0.9857  val_loss 0.0794  val_acc 0.9756\n",
      "Epoch 6  train_loss 0.0449  train_acc 0.9899  val_loss 0.0721  val_acc 0.9777\n",
      "Epoch 7  train_loss 0.0350  train_acc 0.9927  val_loss 0.0709  val_acc 0.9773\n",
      "Epoch 8  train_loss 0.0271  train_acc 0.9954  val_loss 0.0686  val_acc 0.9778\n",
      "Epoch 9  train_loss 0.0213  train_acc 0.9967  val_loss 0.0642  val_acc 0.9788\n",
      "Epoch 10  train_loss 0.0165  train_acc 0.9981  val_loss 0.0629  val_acc 0.9799\n",
      "Epoch 11  train_loss 0.0127  train_acc 0.9988  val_loss 0.0619  val_acc 0.9792\n",
      "Epoch 12  train_loss 0.0100  train_acc 0.9993  val_loss 0.0630  val_acc 0.9806\n",
      "Epoch 13  train_loss 0.0080  train_acc 0.9995  val_loss 0.0618  val_acc 0.9793\n",
      "Epoch 14  train_loss 0.0059  train_acc 0.9998  val_loss 0.0623  val_acc 0.9798\n",
      "Epoch 15  train_loss 0.0047  train_acc 0.9998  val_loss 0.0601  val_acc 0.9801\n",
      "Epoch 16  train_loss 0.0037  train_acc 0.9999  val_loss 0.0598  val_acc 0.9809\n",
      "Epoch 17  train_loss 0.0029  train_acc 1.0000  val_loss 0.0602  val_acc 0.9819\n",
      "Epoch 18  train_loss 0.0022  train_acc 1.0000  val_loss 0.0606  val_acc 0.9818\n",
      "Epoch 19  train_loss 0.0020  train_acc 1.0000  val_loss 0.0593  val_acc 0.9821\n",
      "Epoch 20  train_loss 0.0014  train_acc 1.0000  val_loss 0.0620  val_acc 0.9820\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "65eaaebffea6c591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4491af01c33a9544"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T01:24:44.432514Z",
     "start_time": "2025-09-15T01:24:43.552829Z"
    }
   },
   "cell_type": "code",
   "source": "tf.range(10)",
   "id": "5a6016e92a15e995",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T01:25:02.126628Z",
     "start_time": "2025-09-15T01:25:02.116805Z"
    }
   },
   "cell_type": "code",
   "source": "tf.constant(np.arange(10))",
   "id": "1ce4a28bc79d5975",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
